{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArshanBhanage/Apache-Beam/blob/main/apache_beam_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80fd4da",
      "metadata": {
        "id": "d80fd4da"
      },
      "source": [
        "\n",
        "# Apache Beam — Colab Notebook\n",
        "**Covers:** composite transform, pipeline I/O, `ParDo`, windowing, `Map`, `Filter`, `Partition`\n",
        "\n",
        "This notebook demonstrates core Apache Beam features in Python on the DirectRunner. It includes:\n",
        "\n",
        "- **Batch pipeline** reading from text, cleaning & tokenizing, then writing outputs\n",
        "- **Composite transform** (`PTransform`) that chains multiple transforms\n",
        "- **Elementwise transforms**: `Map`, `Filter`, `FlatMap`\n",
        "- **`ParDo`** using a custom `DoFn`\n",
        "- **`Partition`** to split a `PCollection` into multiple categories\n",
        "- **Pipeline I/O** via `ReadFromText` and `WriteToText`\n",
        "- **Streaming-style windowing** example using `GenerateSequence` with fixed windows + early triggering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f3e5d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c9f3e5d3",
        "outputId": "f3608095-ec24-429f-be4d-186748fba841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install \"apache-beam[gcp]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7db64a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7db64a3",
        "outputId": "56a6c76c-35e2-4ffe-cfcb-bd55cc9f0e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created sample input file at: /content/data/sentences.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, re, glob, json, time, shutil, random, string\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.argv = sys.argv[:1]\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
        "from apache_beam.transforms.window import FixedWindows\n",
        "from apache_beam.transforms.trigger import AfterWatermark, AccumulationMode, AfterProcessingTime, Repeatedly, AfterCount\n",
        "\n",
        "DATA_DIR = Path('/content/data')\n",
        "OUT_DIR  = Path('/content/output')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "text_path = DATA_DIR / 'sentences.txt'\n",
        "text_path.write_text(\n",
        "    \"Apache Beam makes data processing portable and scalable.\\n\"\n",
        "    \"This is a tiny local dataset for our Beam assignment.\\n\"\n",
        "    \"Windowing lets us reason about unbounded event streams.\\n\"\n",
        "    \"ParDo DoFn can output zero, one, or many elements.\\n\"\n",
        "    \"Map, Filter, and Partition are handy elementwise transforms.\\n\"\n",
        ")\n",
        "\n",
        "print(f\"Created sample input file at: {text_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edef7e84",
      "metadata": {
        "id": "edef7e84"
      },
      "source": [
        "\n",
        "## Composite Transform: `CleanAndTokenize`\n",
        "This `PTransform` chains `Map`/`FlatMap`/`Filter` steps to lowercase, strip punctuation, and split lines into words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b7e252",
      "metadata": {
        "id": "54b7e252"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CleanAndTokenize(beam.PTransform):\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | \"Lowercase\" >> beam.Map(lambda s: s.lower())\n",
        "            | \"OnlyLetters\" >> beam.Map(lambda s: re.sub(r\"[^a-z\\s]\", \" \", s))\n",
        "            | \"SplitWords\" >> beam.FlatMap(lambda s: s.split())\n",
        "            | \"FilterEmpty\" >> beam.Filter(lambda w: bool(w.strip()))\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf4f58e1",
      "metadata": {
        "id": "cf4f58e1"
      },
      "source": [
        "\n",
        "## `ParDo` with a custom `DoFn`\n",
        "We tag each word with a simple category (short/medium/long) and emit `(category, word)` pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f425faf",
      "metadata": {
        "id": "3f425faf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TagWordLength(beam.DoFn):\n",
        "    def process(self, word: str):\n",
        "        L = len(word)\n",
        "        if L < 4:\n",
        "            tag = \"short\"\n",
        "        elif L < 8:\n",
        "            tag = \"medium\"\n",
        "        else:\n",
        "            tag = \"long\"\n",
        "        # Emit a key-value pair so we can group or count later.\n",
        "        yield (tag, word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59220791",
      "metadata": {
        "id": "59220791"
      },
      "source": [
        "\n",
        "## Batch Pipeline (Text I/O, Map, Filter, ParDo, Partition, WriteToText)\n",
        "This pipeline:\n",
        "1. **Reads** lines from our local text file (pipeline I/O).\n",
        "2. Applies the **composite transform** to clean & tokenize.\n",
        "3. Uses **Map** to pair each word with the number 1, and **Filter** to drop ultra-short tokens.\n",
        "4. Runs a **ParDo** (`TagWordLength`) to categorize each word.\n",
        "5. Uses **Partition** to split words into *short*, *medium*, and *long* groups.\n",
        "6. **Writes** results to `/content/output/` as text files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330b0c7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "330b0c7a",
        "outputId": "b78e4e0e-4fc9-4f0e-83b2-ddce56dbba7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-2aea6fc8-c968-4f7f-ad60-72930848a0be.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-2aea6fc8-c968-4f7f-ad60-72930848a0be.json']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-2aea6fc8-c968-4f7f-ad60-72930848a0be.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch pipeline finished. Check /content/output for results.\n"
          ]
        }
      ],
      "source": [
        "def by_length_partition(word, n_partitions):\n",
        "    L = len(word)\n",
        "    if L < 4:\n",
        "        return 0  # short\n",
        "    elif L < 8:\n",
        "        return 1  # medium\n",
        "    else:\n",
        "        return 2  # long\n",
        "\n",
        "batch_output_prefix = str(OUT_DIR / \"batch_words\")\n",
        "\n",
        "# Clean old outputs\n",
        "for f in glob.glob(batch_output_prefix + \"*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "options = PipelineOptions(\n",
        "    save_main_session=True,\n",
        ")\n",
        "with beam.Pipeline(options=options) as p:\n",
        "    lines = p | \"ReadText\" >> beam.io.ReadFromText(str(text_path))\n",
        "\n",
        "    words = (\n",
        "        lines\n",
        "        | \"CleanAndTokenize\" >> CleanAndTokenize()\n",
        "        | \"DropTiny\" >> beam.Filter(lambda w: len(w) >= 2)   # Filter\n",
        "    )\n",
        "\n",
        "    # Map: (word, 1)\n",
        "    word_ones = words | \"PairWithOne\" >> beam.Map(lambda w: (w, 1))\n",
        "\n",
        "    # ParDo: tag by length -> (tag, word)\n",
        "    tagged = words | \"TagWithParDo\" >> beam.ParDo(TagWordLength())\n",
        "\n",
        "    # Partition into 3 buckets\n",
        "    short, medium, long = words | \"PartitionByLength\" >> beam.Partition(by_length_partition, 3)\n",
        "\n",
        "    # Write out each part\n",
        "    _ = short  | \"WriteShort\"  >> beam.io.WriteToText(batch_output_prefix + \"_short\")\n",
        "    _ = medium | \"WriteMedium\" >> beam.io.WriteToText(batch_output_prefix + \"_medium\")\n",
        "    _ = long   | \"WriteLong\"   >> beam.io.WriteToText(batch_output_prefix + \"_long\")\n",
        "\n",
        "    # Also produce a simple word count and write to text\n",
        "    counts = (\n",
        "        word_ones\n",
        "        | \"CombineCounts\" >> beam.CombinePerKey(sum)\n",
        "        | \"FormatCounts\"  >> beam.Map(lambda kv: f\"{kv[0]}\t{kv[1]}\")\n",
        "        | \"WriteCounts\"   >> beam.io.WriteToText(batch_output_prefix + \"_counts\")\n",
        "    )\n",
        "\n",
        "print(\"Batch pipeline finished. Check /content/output for results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fd303a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49fd303a",
        "outputId": "840036c8-9e9d-415f-c81d-7bec6d7e45d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Batch Outputs ===\n",
            "\n",
            "--- /content/output/batch_words_counts-00000-of-00001 ---\n",
            "apache\t1\n",
            "beam\t2\n",
            "makes\t1\n",
            "data\t1\n",
            "processing\t1\n",
            "portable\t1\n",
            "and\t2\n",
            "scalable\t1\n",
            "this\t1\n",
            "is\t1\n",
            "...\n",
            "\n",
            "--- /content/output/batch_words_long-00000-of-00001 ---\n",
            "processing\n",
            "portable\n",
            "scalable\n",
            "assignment\n",
            "windowing\n",
            "unbounded\n",
            "elements\n",
            "partition\n",
            "elementwise\n",
            "transforms\n",
            "...\n",
            "\n",
            "--- /content/output/batch_words_medium-00000-of-00001 ---\n",
            "apache\n",
            "beam\n",
            "makes\n",
            "data\n",
            "this\n",
            "tiny\n",
            "local\n",
            "dataset\n",
            "beam\n",
            "lets\n",
            "...\n",
            "\n",
            "--- /content/output/batch_words_short-00000-of-00001 ---\n",
            "and\n",
            "is\n",
            "for\n",
            "our\n",
            "us\n",
            "can\n",
            "one\n",
            "or\n",
            "map\n",
            "and\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=== Batch Outputs ===\")\n",
        "for path in sorted(glob.glob(str(OUT_DIR / \"batch_words*\"))):\n",
        "    print(\"\\n---\", path, \"---\")\n",
        "    try:\n",
        "        with open(path) as f:\n",
        "            for i, line in enumerate(f):\n",
        "                print(line.rstrip())\n",
        "                if i > 8:\n",
        "                    print(\"...\")\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        # Some outputs may be sharded without .txt extension depending on runner;\n",
        "        # try listing file names only.\n",
        "        print(\"Could not open file directly; listed for reference.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8000c34d",
      "metadata": {
        "id": "8000c34d"
      },
      "source": [
        "\n",
        "## Streaming-Style Windowing (Fixed windows + Early triggers)\n",
        "Here we simulate a small unbounded stream using `GenerateSequence` that emits 5 elements/sec up to 50 elements total.  \n",
        "We apply **fixed windows** of 10 seconds, set **early triggers** so partial results appear early, and then **count** elements per window.\n",
        "\n",
        "We also format the output with each element's window start time for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93bdd01a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93bdd01a",
        "outputId": "ebbebc58-79f8-455c-eda1-b67f166ca2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming windowing pipeline finished. See /content/output for stream results.\n"
          ]
        }
      ],
      "source": [
        "class FormatWithWindow(beam.DoFn):\n",
        "    def process(self, kv, window=beam.DoFn.WindowParam):\n",
        "        key, value = kv\n",
        "        start = window.start.to_utc_datetime().isoformat()\n",
        "        end   = window.end.to_utc_datetime().isoformat()\n",
        "        yield json.dumps({\"window_start\": start, \"window_end\": end, key: value})\n",
        "\n",
        "stream_output_prefix = str(OUT_DIR / \"stream_counts\")\n",
        "\n",
        "# Clean old outputs\n",
        "for f in glob.glob(stream_output_prefix + \"*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "stream_opts = PipelineOptions(\n",
        "    streaming=True,\n",
        "    save_main_session=True,\n",
        ")\n",
        "with beam.Pipeline(options=stream_opts) as p:\n",
        "    # Create 50 sequential integers to simulate a stream\n",
        "    events = (\n",
        "        p\n",
        "        | \"CreateSequence\" >> beam.Create(range(50))\n",
        "        | \"StampEventTime\" >> beam.Map(lambda i: beam.window.TimestampedValue(i, time.time() + i))\n",
        "\n",
        "    )\n",
        "\n",
        "    windowed_counts = (\n",
        "        events\n",
        "        | \"ToOnes\" >> beam.Map(lambda x: (\"count\", 1))\n",
        "        | \"Fixed10s\" >> beam.WindowInto(\n",
        "            FixedWindows(10),\n",
        "            trigger=AfterWatermark(early=AfterCount(5)),\n",
        "            accumulation_mode=AccumulationMode.DISCARDING\n",
        "        )\n",
        "        | \"SumPerWindow\" >> beam.CombinePerKey(sum)\n",
        "        | \"FormatWin\" >> beam.ParDo(FormatWithWindow())\n",
        "        | \"WriteStreamOut\" >> beam.io.WriteToText(stream_output_prefix)\n",
        "    )\n",
        "\n",
        "print(\"Streaming windowing pipeline finished. See /content/output for stream results.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f5164f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f5164f",
        "outputId": "c1a2a87f-a786-4de2-d4d5-5b3dd22b3bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Streaming Outputs ===\n",
            "\n",
            "--- /content/output/stream_counts-00000-of-00001 ---\n",
            "{\"window_start\": \"2025-10-19T23:28:50\", \"window_end\": \"2025-10-19T23:29:00\", \"count\": 6}\n",
            "{\"window_start\": \"2025-10-19T23:29:00\", \"window_end\": \"2025-10-19T23:29:10\", \"count\": 10}\n",
            "{\"window_start\": \"2025-10-19T23:29:10\", \"window_end\": \"2025-10-19T23:29:20\", \"count\": 10}\n",
            "{\"window_start\": \"2025-10-19T23:29:20\", \"window_end\": \"2025-10-19T23:29:30\", \"count\": 10}\n",
            "{\"window_start\": \"2025-10-19T23:29:30\", \"window_end\": \"2025-10-19T23:29:40\", \"count\": 10}\n",
            "{\"window_start\": \"2025-10-19T23:28:50\", \"window_end\": \"2025-10-19T23:29:00\", \"count\": 0}\n",
            "{\"window_start\": \"2025-10-19T23:29:00\", \"window_end\": \"2025-10-19T23:29:10\", \"count\": 0}\n",
            "{\"window_start\": \"2025-10-19T23:29:10\", \"window_end\": \"2025-10-19T23:29:20\", \"count\": 0}\n",
            "{\"window_start\": \"2025-10-19T23:29:20\", \"window_end\": \"2025-10-19T23:29:30\", \"count\": 0}\n",
            "{\"window_start\": \"2025-10-19T23:29:30\", \"window_end\": \"2025-10-19T23:29:40\", \"count\": 0}\n",
            "{\"window_start\": \"2025-10-19T23:29:40\", \"window_end\": \"2025-10-19T23:29:50\", \"count\": 4}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=== Streaming Outputs ===\")\n",
        "for path in sorted(glob.glob(str(OUT_DIR / \"stream_counts*\"))):\n",
        "    print(\"\\n---\", path, \"---\")\n",
        "    try:\n",
        "        with open(path) as f:\n",
        "            for i, line in enumerate(f):\n",
        "                print(line.rstrip())\n",
        "                if i > 12:\n",
        "                    print(\"...\")\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(\"Could not open file directly; listed for reference.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3877731c",
      "metadata": {
        "id": "3877731c"
      },
      "source": [
        "\n",
        "### (Optional) Bonus: Side Input example\n",
        "We pass a set of stopwords as a side input to filter tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c73f2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c73f2f",
        "outputId": "b113eab3-2c07-4f6b-a8ec-dc5b992e5539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Side input demo complete. Check /content/output for 'side_input_demo*' files.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "side_output_prefix = str(OUT_DIR / \"side_input_demo\")\n",
        "for f in glob.glob(side_output_prefix + \"*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "stopwords = {\"a\", \"an\", \"the\", \"and\", \"or\", \"is\", \"of\", \"to\", \"us\"}\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions(save_main_session=True)) as p:\n",
        "    lines = p | beam.io.ReadFromText(str(text_path))\n",
        "    words = lines | CleanAndTokenize()\n",
        "\n",
        "    filtered = (\n",
        "        words\n",
        "        | beam.Filter(lambda w, sw: w not in sw, sw=beam.pvalue.AsSingleton(p | beam.Create([stopwords])))\n",
        "        | beam.io.WriteToText(side_output_prefix)\n",
        "    )\n",
        "\n",
        "print(\"Side input demo complete. Check /content/output for 'side_input_demo*' files.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}